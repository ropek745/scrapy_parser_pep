# Проект: асинхронный парсер PEP

### Описание
Парсер сайта [PEP](https://peps.python.org/), написанный на асинхронном фреймворке [Scrapy](https://docs.scrapy.org/en/latest/index.html).

Данный парсер позволяет:
 - получить информацию обо всех PEP (номер, название, статус);
 - получить информацию об общем кол-ве статусов.
 
Полученные данные предоставляются в формате ```.csv```

 ### Используемые технологии
  - Python 3.9
  - Scrapy 2.7

### Порядок запуска
1. Клонировать проект.
```
git@github.com:ropek745/scrapy_parser_pep.git
```
2. Создать и активировать виртуальное окружение. Установить зависимости.
```
python -m venv venv #(for Windows)
```
```
python3 -m venv venv #(for MacOs/ Linux)
```
```
python -m pip install --upgrade pip
```
```
pip install -r requirements.txt
```
3. Запустить парсер
```
scrapy crawl pep
```

### Результаты

После ввода команды ```scrapy crawl pep``` результатом работы программы будут два файла:
 - pep_{время_создания}.csv. Файл со списком все номерами всех PEP, их названиями и статусами;
 - summary_statuse_{время_создания}.csv. Файл суммарным кол-вом всех возможных статусов PEP.

Данные файлы появятся в директории ```results/```. 



## Разработчик - [Роман Пекарев](https://github.com/ropek745) ##

